---
title: "The Average Goals per Player per Season in the NHL (2000-2018)"
#author: "Fall 2020, JMN420: Jeffrey Yao, Martha Dixon, Nisha Sen"
#date: "Due: Monday, December 14"
output: 
  html_document:
    theme: readable
    toc: yes
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
```


# Introduction

The dataset that we used in this report can be found [here](http://inalitic.com/datasets/nhl%20player%20data.html).

# Methods

First, we need to load the dataset, and we can take a look at the first and last few observations and name all of the variables.
```{r}
skaters_full = read.csv("skater_stats.csv")
head(skaters_full)
tail(skaters_full)
names(skaters_full)
```

This data set in its entirety is quite large, and encompasses a large time frame, from 1940 until 2018. In order to make this more relevant, we decided to subset the data and only consider observations between 2000 and 2018, as follows.

```{r}
skaters <- subset(skaters_full, Season >= 2000)
```

Next, looking at the structure of our data frame, we can see that we have:

- a lot of numerical observations that R views as character observations, and
- a lot of empty values within each category, which become NA values once we convert variables to numeric. 

```{r}
str(skaters)
```


We amend this by converting the relevant vectors to numeric and setting all of the NA values equal to 0. We can see how this changes the dataset by again checking the structure.

```{r, warning = FALSE}
skaters$Pos <- as.factor(skaters$Pos)
skaters$Age <- as.numeric(skaters$Age)
skaters$GP <- as.numeric(skaters$GP)
skaters$G <- as.numeric(skaters$G)
skaters$GPG <- as.numeric(skaters$GPG)
skaters$A <- as.numeric(skaters$A)
skaters$PTS <- as.numeric(skaters$PTS)
skaters$X... <- as.numeric(skaters$X...)
skaters$PIM <- as.numeric(skaters$PIM)
skaters$EVG <- as.numeric(skaters$EVG)
skaters$PPG <- as.numeric(skaters$PPG)
skaters$SHG <- as.numeric(skaters$SHG)
skaters$GWG <- as.numeric(skaters$GWG)
skaters$EVA <- as.numeric(skaters$EVA)
skaters$PPA <- as.numeric(skaters$PPA)
skaters$SHA <- as.numeric(skaters$SHA)
skaters$S <- as.numeric(skaters$S)
skaters$S. <- as.numeric(skaters$S.)
skaters$TOI <- as.numeric(skaters$TOI)
skaters$BLK <- as.numeric(skaters$BLK)
skaters$HIT <- as.numeric(skaters$HIT)
skaters$FOwin <- as.numeric(skaters$FOwin)
skaters$FOloss <- as.numeric(skaters$FOloss)
skaters$FO. <- as.numeric(skaters$FO.)
```

```{r}
skaters[is.na(skaters)] = 0
str(skaters)
```

As we can see, we have a lot of different predictors to choose from when we consider how to best model the average goals per player. The response variable we are considering, Goals (`G`), is used in the calculations of a few other variables, so we exclude these variables from our consideration. These variables include: 

- `PTS`, points tallied by that player in corresponding season; `PTS` = `G` + `A`
- `GPG`, goals per game by that player in corresponding season
- `EVG`, goals scored by player while team is even strength
- `PPG`, goals scored by player while team is on the powerplay
- `SHG`, goals scored by player while team is on the penalty kill
- `GWG`, game winning goals scored by player in that season
- `S.` or S%, the shooting percentage of player, which is calculated from goals scored (G) divided by the shots taken (S)

We also exclude the variable `X` from our analysis, since it is not included as a variable in the source documentation for our data; this column exists as a descriptor and is a combination of `Season` and `Player`.

We take this into account below, and remove these variables:

```{r}
skaters_cleaned <- skaters[, c(4, 6, 7, 8, 10, 13, 21, 25, 26, 27, 28)]

```

The full additive model(excluding the variables above) can be found in the **Appendix** section.

Before we started testing models, we needed to make sure we were able to check and validate assumptions. We used the following functions to do so graphically:

```{r}
plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}

```

Every model we fitted violated the assumptions of constant variance and of normally distributed errors; we attempted to fit the full additive model, a polynomial (squared) model, and models found using AIC based on both of those (code can be found in **Appendix**). We also attempted to transform the predictors using logarithmic transformations, square root transformations, and cube root transformations (omitted for relevance), but all of those models violated assumptions as well.

After all of this investigation, we decided to select the model we found with the greatest $R^2$ value, which ended up being our polynomial model, or `goals_bigModel`. The adjusted $R^2$ value from the model we found using backwards AIC on this model was the same as the $R^2$ value of this model itself. The model is as follows, along with its $R^2$ value and our attempt to check assumptions:

```{r}
goals_bigModel = lm(G ~. ^ 2 + I(Age ^ 2)  + I(GP ^ 2) + I(A ^ 2) + I(PIM ^ 2) + I(S ^ 2) + I(BLK ^ 2) + I(FOwin ^ 2) + I(FOloss ^ 2), data = skaters_cleaned)
summary(goals_bigModel)$r.sq
```

```{r}
library(lmtest)
plot_fitted_resid(goals_bigModel)
bptest(goals_bigModel)
plot_qq(goals_bigModel)
```


# Results

# Discussion

# Appendix

The full additive model:

```{r}
goals_addfull = lm(G ~ ., data = skaters_cleaned)
summary(goals_addfull)$r.sq
```

Interaction models took too long to run with RStudio (even models that used only 2 predictors), so we excluded those from our analysis as well.

In our analysis, we used backwards selection procedures with AIC as our metric, starting on the full additive model:

```{r}
goals_model_back_aic = step(goals_addfull, direction = "backward", trace = 0)
summary(goals_model_back_aic)$adj.r.sq

```

We also searched through the possible models using BIC; the code is included below but will not run when knitting together the .Rmd file, in order to maximize processing efficiency:

```{r, eval=FALSE}
n = length(resid(goals_addfull))
goals_model_back_bic = step(goals_addfull, direction = "backward", k = log(n))
summary(goals_model_back_bic)adj.$r.sq
```

We can see that the model found using AIC is slightly better than the model found using BIC, but the full additive model still yields the highest $R^2$ value.

We considered a polynomial model, with backwards AIC as well (not run to minimize the time taken to knit an html file):

```{r, eval=FALSE}
goals_bigModel = lm(G ~. ^ 2 + I(Age ^ 2)  + I(GP ^ 2) + I(A ^ 2) + I(PIM ^ 2) + I(S ^ 2) + I(BLK ^ 2) + I(FOwin ^ 2) + I(FOloss ^ 2), data = skaters_cleaned)
summary(goals_bigModel)$r.sq
big_model_back_aic = step(goals_bigModel, direction = "backward", trace = 0)
summary(big_model_back_aic)$r.sq
```

Code is not included here for concision, but all assumptions were violated when we attempted to check validity of the assumptions for constant variance and normality of errors, using a combination of the functions included in the **Methods** section and the `bptest()` function.

### Fall 2020, JMN420: Jeffrey Yao, Martha Dixon, Nisha Sen